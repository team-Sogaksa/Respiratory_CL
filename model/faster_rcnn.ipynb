{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dff683-3eee-4e39-bd4b-175a3492f8f4",
   "metadata": {},
   "source": [
    "# faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7369ae24-a26c-408a-aae0-4508e0e43757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e228d4-f517-4a99-8cb6-10cd03a3bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f8be0f-5700-4113-9265-0c9a821bf90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 지정\n",
    "all_xml_dir = '/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/with_normal_224/xml_files'\n",
    "\n",
    "# 원본 이미지\n",
    "dir_train = \"/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/model/Yolov8/dataset/Yolo8_224/Training/data/train/images/\"\n",
    "dir_val = \"/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/model/Yolov8/dataset/Yolo8_224/Training/data/val/images/\"\n",
    "dir_test = \"/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/model/Yolov8/dataset/Yolo8_224/Training/data/test/images/\"\n",
    "\n",
    "# xml 저장 디렉토리\n",
    "dir_train_xml = \"/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/model/Yolov8/dataset/Yolo8_224/Training/data/train/xmls/\"\n",
    "dir_val_xml = \"/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/model/Yolov8/dataset/Yolo8_224/Training/data/val/xmls/\"\n",
    "dir_test_xml = \"/home/sogaksa123/AIFFEL_THON/data/dataset4_0331/model/Yolov8/dataset/Yolo8_224/Training/data/test/xmls/\"\n",
    "\n",
    "# csv 저장 디렉토리\n",
    "csv_train = '/home/sogaksa123/AIFFEL_THON/ex/csv_train'\n",
    "csv_val = '/home/sogaksa123/AIFFEL_THON/ex/csv_val'\n",
    "csv_test = '/home/sogaksa123/AIFFEL_THON/ex/csv_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72eb86ae-2a59-4a68-8eeb-2ab02c635d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv로 df 만들어줌\n",
    "box_train = pd.read_csv(csv_train)\n",
    "box_val = pd.read_csv(csv_val)\n",
    "box_test = pd.read_csv(csv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178e8e74-f93d-4ae5-856c-5c5c374a8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    '''train_ds = Dataset(box, dir_train)'''\n",
    "    def __init__(self, df, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_ids = df[\"Image_Name\"].unique() # all image filenames(겹치지 않게. 2978개, list array)\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir # dir to image files\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        '''\n",
    "        image_id: filename dataframe. 2978개\n",
    "        records: 하나의 이미지에 포함된 박스들의 df\n",
    "        image: filename(image_id)이미지 불러옴\n",
    "        '''\n",
    "        image_id = self.image_ids[idx]\n",
    "        records = self.df[self.df[\"Image_Name\"] == image_id] # 하나의 이미지에 포함된 박스들을 records라는 df로 \n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.image_dir, image_id), cv2.IMREAD_COLOR)\n",
    "        heights, widths = image.shape[:2]\n",
    "        # torch에서 사용할 수 있게 바꿔줌\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0 # normalization\n",
    "        image = torch.tensor(image) # ndarray to tensor\n",
    "        image = image.permute(2,0,1) # 차원 순서 변경\n",
    "        \n",
    "        # 박스 좌표값들만 array로 반환\n",
    "        boxes = records[[\"X_min\", \"Y_min\", \"X_max\", \"Y_max\"]].values\n",
    "        # 박스들 전체 면적을 tensor로 (box3개 -> tesnsor[3,]\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        '''   height = ymax - ymin           width = xmax - xmin      '''\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        \n",
    "        masks = []\n",
    "        for box in boxes:\n",
    "            # 박스 크기에 맞는 ndarray 생성\n",
    "            mask = np.zeros([int(heights), int(widths)], np.uint8)\n",
    "            # 박스값 할당\n",
    "            masks.append(cv2.rectangle(mask, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), 1, -1))\n",
    "        # array to tensor\n",
    "        masks = torch.tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        label_ls = []\n",
    "        for c in records['Label']:\n",
    "            if c == 'crackles':\n",
    "                classes = 0\n",
    "            elif c == 'wheezes':\n",
    "                classes = 1\n",
    "            elif c == 'normal':\n",
    "                classes = 2\n",
    "            label_ls.append(classes)\n",
    "        labels = torch.tensor(label_ls, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.tensor(boxes)\n",
    "        target[\"labels\"] = labels\n",
    "        target['masks'] = masks\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "\n",
    "        num_objs = len(label_ls) \n",
    "        target[\"iscrowd\"] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # augmentation(지금은 None)\n",
    "        if self.transforms:\n",
    "            sample = {\"image\": image, \"boxes\": target[\"boxes\"], \"labels\": labels}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample[\"image\"]\n",
    "            target[\"boxes\"] = torch.stack(tuple(map(torch.tensor, zip(*sample[\"boxes\"])))).permute(1, 0)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7c06eb-c9af-4eb4-a6ee-20a11116182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "train_ds = Dataset(box_train, dir_train)\n",
    "val_ds = Dataset(box_val, dir_val)\n",
    "test_ds = Dataset(box_test, dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94320f6a-d5d9-41b4-ac1c-ee38c35bd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10004da-63da-4758-a3ce-c3b9eb6a8b74",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fa94f6-26b7-40df-b97a-5f3d7ed16ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "  \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001a5ef4-3c05-4b68-865d-bc0ee90d123f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(4)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c89d6-83c6-4ea6-a013-581f70ea954d",
   "metadata": {},
   "source": [
    "### parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259af599-6371-45c4-bec4-600270dfdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36082495-080d-4324-be4a-d9ea149d9250",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e23b296-46a9-4425-8b70-c8b9081a0cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b702fe82-e547-4f4b-b799-8178f4f62b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------train start--------------------------\n",
      "epoch : 1, Loss : 54.321231842041016, time : 206.76458048820496\n",
      "epoch : 2, Loss : 37.4193115234375, time : 204.99448108673096\n",
      "epoch : 3, Loss : 33.554840087890625, time : 204.97400736808777\n",
      "epoch : 4, Loss : 32.03372573852539, time : 204.95419478416443\n",
      "epoch : 5, Loss : 30.220813751220703, time : 205.11805772781372\n",
      "epoch : 6, Loss : 28.233070373535156, time : 204.92293787002563\n",
      "epoch : 7, Loss : 26.77448844909668, time : 204.62311434745789\n",
      "epoch : 8, Loss : 26.179807662963867, time : 204.6527214050293\n",
      "epoch : 9, Loss : 24.72422981262207, time : 204.5974633693695\n",
      "epoch : 10, Loss : 23.199947357177734, time : 204.73705387115479\n",
      "epoch : 11, Loss : 22.408634185791016, time : 204.79533863067627\n",
      "epoch : 12, Loss : 21.66169548034668, time : 204.7125153541565\n",
      "epoch : 13, Loss : 20.837278366088867, time : 204.99058747291565\n",
      "epoch : 14, Loss : 20.633699417114258, time : 204.91739130020142\n",
      "epoch : 15, Loss : 19.22830581665039, time : 205.01649641990662\n",
      "epoch : 16, Loss : 18.526517868041992, time : 204.83595204353333\n",
      "epoch : 17, Loss : 18.530197143554688, time : 204.95274901390076\n",
      "epoch : 18, Loss : 16.91036605834961, time : 204.91520810127258\n",
      "epoch : 19, Loss : 17.426279067993164, time : 204.45183944702148\n",
      "epoch : 20, Loss : 15.971455574035645, time : 204.46624374389648\n",
      "epoch : 23, Loss : 14.241006851196289, time : 204.91945242881775\n",
      "epoch : 24, Loss : 13.699174880981445, time : 205.0922029018402\n",
      "epoch : 25, Loss : 12.768804550170898, time : 205.4121789932251\n",
      "epoch : 26, Loss : 12.959830284118652, time : 205.14256191253662\n",
      "epoch : 27, Loss : 11.906084060668945, time : 204.96665287017822\n",
      "epoch : 28, Loss : 12.020224571228027, time : 205.353520154953\n",
      "epoch : 29, Loss : 10.919391632080078, time : 204.73630738258362\n",
      "epoch : 30, Loss : 10.594615936279297, time : 204.71401023864746\n"
     ]
    }
   ],
   "source": [
    "print('----------------------train start--------------------------')\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in train_dl:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations) \n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        epoch_loss += losses\n",
    "    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84f9557a-c93d-4dce-be34-797566923e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight save\n",
    "torch.save(model.state_dict(),f'model_{num_epochs}.pt')\n",
    "# weight load\n",
    "model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6833b-9ccd-41d4-ab2e-156dc1f563a3",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c488bc11-fdb5-48cd-81ff-449fb15dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7035c02-71d1-4f02-b04e-2ce25571bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 47.4355,  54.7546, 155.9595, 219.0713],\n",
      "        [  0.8857,  53.7017,  54.2913, 218.4235],\n",
      "        [154.6310,  57.1863, 189.6456, 218.2519]], device='cuda:0'), 'labels': tensor([2, 2, 2], device='cuda:0'), 'scores': tensor([0.9636, 0.9215, 0.8861], device='cuda:0')}, {'boxes': tensor([[116.1536,  55.9697, 220.0185, 217.8556],\n",
      "        [  3.4093,  56.1787, 124.5192, 218.2382]], device='cuda:0'), 'labels': tensor([2, 2], device='cuda:0'), 'scores': tensor([0.8441, 0.8394], device='cuda:0')}, {'boxes': tensor([[  0.6995,  55.8153,  74.9493, 219.1944],\n",
      "        [161.4565,  54.0115, 224.0000, 219.3746],\n",
      "        [ 72.8456,  58.9890, 161.4222, 217.7514],\n",
      "        [  0.6279,  57.0063,  30.7992, 219.3555]], device='cuda:0'), 'labels': tensor([2, 2, 2, 2], device='cuda:0'), 'scores': tensor([0.9826, 0.9476, 0.9414, 0.7894], device='cuda:0')}, {'boxes': tensor([[  0.0000,  55.7511,  54.5792, 218.7144],\n",
      "        [ 84.2990,  54.4633, 161.4490, 217.3032],\n",
      "        [159.7381,  53.4695, 189.1541, 219.3229]], device='cuda:0'), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.9014, 0.6074, 0.5099], device='cuda:0')}, {'boxes': tensor([[1.5975e-01, 5.2960e+01, 2.1030e+01, 2.1787e+02],\n",
      "        [2.1452e+01, 5.6336e+01, 1.7533e+02, 2.1800e+02]], device='cuda:0'), 'labels': tensor([2, 2], device='cuda:0'), 'scores': tensor([0.7636, 0.7451], device='cuda:0')}, {'boxes': tensor([[ 21.1863,  57.2186, 101.9926, 218.5764],\n",
      "        [103.4154,  55.6101, 223.2833, 219.5748],\n",
      "        [ 99.0985,  56.5712, 163.3390, 218.6386]], device='cuda:0'), 'labels': tensor([2, 2, 2], device='cuda:0'), 'scores': tensor([0.9961, 0.9558, 0.7576], device='cuda:0')}, {'boxes': tensor([[125.9323,  56.3303, 214.5382, 217.2417],\n",
      "        [212.7637,  57.9301, 223.2285, 217.2596],\n",
      "        [ 53.6054,  56.2910, 168.4090, 219.1080],\n",
      "        [134.0023,  59.0278, 211.4677, 209.6574]], device='cuda:0'), 'labels': tensor([1, 1, 1, 2], device='cuda:0'), 'scores': tensor([0.8926, 0.7753, 0.6840, 0.5943], device='cuda:0')}, {'boxes': tensor([[ 58.3926,  55.5970, 224.0000, 217.1122]], device='cuda:0'), 'labels': tensor([2], device='cuda:0'), 'scores': tensor([0.7956], device='cuda:0')}, {'boxes': tensor([[ 39.1410,  54.2135, 149.2608, 218.6583],\n",
      "        [102.4375,  54.3144, 215.9257, 218.0435]], device='cuda:0'), 'labels': tensor([2, 2], device='cuda:0'), 'scores': tensor([0.8089, 0.5708], device='cuda:0')}, {'boxes': tensor([[212.5247,  55.5865, 224.0000, 219.4454],\n",
      "        [ 35.9505,  55.7855, 208.2076, 218.3534],\n",
      "        [133.4044,  56.1123, 213.3168, 219.3401]], device='cuda:0'), 'labels': tensor([2, 1, 2], device='cuda:0'), 'scores': tensor([0.6624, 0.6241, 0.5658], device='cuda:0')}, {'boxes': tensor([[ 33.6449,  56.6187, 174.7783, 218.4439],\n",
      "        [  0.0000,  54.4375,  35.6016, 221.4895]], device='cuda:0'), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9846, 0.9656], device='cuda:0')}, {'boxes': tensor([[162.9467,  57.4861, 223.6662, 218.0297],\n",
      "        [ 11.7957,  54.5105, 167.0763, 216.3430]], device='cuda:0'), 'labels': tensor([2, 1], device='cuda:0'), 'scores': tensor([0.8513, 0.5131], device='cuda:0')}, {'boxes': tensor([[120.6990,  55.0727, 224.0000, 220.3857],\n",
      "        [  0.0000,  54.9993, 136.7549, 219.8065],\n",
      "        [  0.7775,  56.1375, 126.5920, 218.0854],\n",
      "        [121.3333,  53.6793, 224.0000, 220.5155]], device='cuda:0'), 'labels': tensor([1, 1, 2, 2], device='cuda:0'), 'scores': tensor([0.9467, 0.9142, 0.6207, 0.6074], device='cuda:0')}, {'boxes': tensor([[140.4776,  55.3102, 205.9490, 218.0527],\n",
      "        [209.7449,  55.2798, 223.9417, 217.4681],\n",
      "        [ 77.2070,  54.3260, 140.0216, 218.3353]], device='cuda:0'), 'labels': tensor([2, 2, 2], device='cuda:0'), 'scores': tensor([0.9678, 0.8440, 0.8082], device='cuda:0')}, {'boxes': tensor([[  1.1912,  55.7828, 113.9299, 219.1136],\n",
      "        [132.8306,  56.7808, 220.4524, 218.9665],\n",
      "        [214.5005,  58.3788, 223.8930, 219.2282]], device='cuda:0'), 'labels': tensor([2, 2, 2], device='cuda:0'), 'scores': tensor([0.9646, 0.8727, 0.5968], device='cuda:0')}, {'boxes': tensor([[126.7933,  55.4045, 211.7929, 218.3608],\n",
      "        [209.5230,  55.9201, 223.8430, 218.6340],\n",
      "        [  1.9064,  55.2888,  78.7104, 218.4761],\n",
      "        [ 73.0629,  55.7716, 128.9022, 219.2610]], device='cuda:0'), 'labels': tensor([2, 2, 2, 2], device='cuda:0'), 'scores': tensor([0.9589, 0.9350, 0.8901, 0.6840], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    # 테스트셋 배치사이즈= 2\n",
    "    for imgs, annotations in test_dl:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "\n",
    "        pred = make_prediction(model, imgs, 0.5)\n",
    "        print(pred)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147fdc8-c2cd-4cd8-94a2-4920f7c7868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _idx = 1\n",
    "# print(\"Target : \", annotations[_idx]['labels'])\n",
    "# plot_image_from_output(imgs[_idx], annotations[_idx])\n",
    "# print(\"Prediction : \", pred[_idx]['labels'])\n",
    "# plot_image_from_output(imgs[_idx], pred[_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ec4a1-f090-4af1-8fbf-2d71d55f469b",
   "metadata": {},
   "source": [
    "## testset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d285c6d6-d01b-4908-915c-3f991ec44d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:23<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels = []\n",
    "preds_adj_all = []\n",
    "annot_all = []\n",
    "\n",
    "for im, annot in tqdm(test_dl, position = 0, leave = True):\n",
    "    im = list(img.to(device) for img in im)\n",
    "    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
    "\n",
    "    for t in annot:\n",
    "        labels += t['labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_adj = make_prediction(model, im, 0.5)\n",
    "        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "        preds_adj_all.append(preds_adj)\n",
    "        annot_all.append(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78bcedca-1fc8-424c-ba5b-4db563f1181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Tutorial-Book-Utils'...\n",
      "remote: Enumerating objects: 45, done.\u001b[K\n",
      "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 45 (delta 18), reused 17 (delta 5), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (45/45), 11.60 KiB | 1.45 MiB/s, done.\n",
      "/home/sogaksa123/AIFFEL_THON/ex/Tutorial-Book-Utils\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n",
    "\n",
    "%cd Tutorial-Book-Utils/\n",
    "import utils_ObjectDetection as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "205e8cec-e73a-4cb6-ac20-6a87aa1fcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP : 0.31641158718916523\n",
      "AP : tensor([0.0000, 0.3408, 0.6084], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "sample_metrics = []\n",
    "for batch_i in range(len(preds_adj_all)):\n",
    "    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "mAP = torch.mean(AP)\n",
    "print(f'mAP : {mAP}')\n",
    "print(f'AP : {AP}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
