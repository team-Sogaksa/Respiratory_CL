{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f811992-3ab7-4c05-a9d8-27195fc2cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict                                            \n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa                                              # image augmentation \n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import xml.etree.ElementTree as ET                                                   # xml 읽어 오는 라이브러리 \n",
    "\n",
    "from xml.etree.ElementTree import Element, ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699db559-714e-47aa-b0eb-c5ab4f5b5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml 함수 정의\n",
    "def xml_parser(xml_path):\n",
    "    xml_path = xml_path\n",
    "    xml = open(xml_path,\"r\")            # xml 열기 \n",
    "    tree = ET.parse(xml)             # tree부분 \n",
    "    root = tree.getroot()            # root \n",
    "    size = root.find('size')            #root 부분에서 size 변수 찾기  \n",
    "    file_name = root.find('filename').text              # filename 찾기 \n",
    "    object_name = []\n",
    "    bbox = []\n",
    "    objects = root.findall('object')\n",
    "    for _object in objects:\n",
    "        name = _object.find('name').text\n",
    "        object_name.append(name)\n",
    "        bndbox = _object.find('bndbox')              #bounding box \n",
    "        one_bbox = []\n",
    "        xmin = bndbox.find(\"xmin\").text            # x좌표 왼쪽부분 \n",
    "        one_bbox.append(int(float(xmin))) \n",
    "        ymin = bndbox.find(\"ymin\").text            # y좌표 왼쪽부분 \n",
    "        one_bbox.append(int(float(ymin)))\n",
    "        xmax = bndbox.find(\"xmax\").text\n",
    "        one_bbox.append(int(float(xmax)))          # x좌표 오른쪽부분 \n",
    "        ymax = bndbox.find(\"ymax\").text\n",
    "        one_bbox.append(int(float(ymax)))          # y좌표 오른쪽부분 \n",
    "        bbox.append(one_bbox)\n",
    "    return file_name, object_name, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "482c351a-e99b-42cd-aca5-1869b6a07a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 내 박스 그리는 함수\n",
    "def makeBox(voc_im,bbox,objects):\n",
    "    image = voc_im.copy()\n",
    "    for i in range(len(objects)):                 # 이미지 내에 box 그리는 함수 \n",
    "        cv2.rectangle(image,(int(bbox[i][0]),int(bbox[i][1])),(int(bbox[i][2]),int(bbox[i][3])),color = (0,255,0),thickness = 1)\n",
    "        cv2.putText(image, objects[i], (int(bbox[i][0]), int(bbox[i][1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a49a982-5a18-4657-9e57-15449d13ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Normal': 1, 'crackles': 2, 'wheezes': 3}\n"
     ]
    }
   ],
   "source": [
    "#라벨 및 객체 정\n",
    "xml_list = os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset5_0403/with_normal_224/xml_files/\")        # xml list directory\n",
    "xml_list.sort()         # file name sort \n",
    "\n",
    "label_set = set() \n",
    "\n",
    "for i in range(len(xml_list)):\n",
    "    xml_path = '/home/sogaksa123/AIFFEL_THON/data/dataset5_0403/with_normal_224/xml_files/' + str(xml_list[i])\n",
    "    file_name, object_name, bbox = xml_parser(xml_path)\n",
    "    for name in object_name:\n",
    "        label_set.add(name)           # object name add (채우기)\n",
    "\n",
    "label_set = sorted(list(label_set))      \n",
    "\n",
    "label_dic = {}\n",
    "for i, key in enumerate(label_set):\n",
    "    label_dic[key] = (i+1)             # label 세 가지  1,2, 3\n",
    "\n",
    "print(label_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ce34d5-b41a-4050-bcc0-91411b92e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용할 데이터 셋 만들\n",
    "class Pascal_Vo(Dataset):\n",
    "    def __init__(self, xml_list, len_data):\n",
    "        \n",
    "        self.xml_list = xml_list\n",
    "        self.len_data = len_data                      # 데이터셋 길이\n",
    "        self.to_tensor = transforms.ToTensor() \n",
    "        self.flip = iaa.Fliplr(0.5)              # augmentation flip 사용 \n",
    "        self.resize = iaa.Resize({'shorter-side': 600, \"longer-side\":\"keep-aspect-ratio\"})   # augmentation resize 사용 \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        xml_path = '/home/sogaksa123/AIFFEL_THON/data/dataset5_0403/with_normal_224/xml_files/' + str(xml_list[idx])\n",
    "        \n",
    "        file_name, object_name, bbox = xml_parser(xml_path)\n",
    "        image_path = '/home/sogaksa123/AIFFEL_THON/data/dataset5_0403/with_normal_224/png_files'+str(file_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        image, bbox = self.flip(image = image, bounding_boxes = np.array([bbox]))\n",
    "        image, bbox = self.resize(image=image, bounding_boxes = bbox)\n",
    "        bbox = bbox.squeeze(0).tolist()\n",
    "        image= self.to_tensor(image)\n",
    "        \n",
    "        targets=[]\n",
    "        d = {}\n",
    "        d['boxes'] = torch.tensor(bbox)\n",
    "        d['labels'] = torch.tensor([label_dic[x] for x in object_name], dtype=torch.int64)\n",
    "        targets.append(d)\n",
    "        \n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0be57ee7-7f66-4353-9346-f9f4255c1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faster RCNN 모델 정의\n",
    "backbone = torchvision.models.vgg16(pretrained=True).features[:-1]    # pretrained model 불러오기 \n",
    "backbone_out = 512              # output size \n",
    "backbone.out_channels = backbone_out   \n",
    "\n",
    "anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(sizes=((128,256,512),),aspect_ratios=((0.5,1.0,2.0),))\n",
    "# detection generator 만들기\n",
    "\n",
    "resolution = 7\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],output_size=resolution,sampling_ratio=2)    # ROi 정의 \n",
    "\n",
    "box_head = torchvision.models.detection.faster_rcnn.TwoMLPHead(in_channels = backbone_out*(resolution**2),representation_size=4096)\n",
    "box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096,4) # class 의 개수\n",
    "\n",
    "# faster rcnn model Linear 부분 + Predictor 부분 \n",
    "\n",
    "\n",
    "model = torchvision.models.detection.FasterRCNN(backbone, num_classes=None,\n",
    "                        min_size= 600, max_size = 1000,\n",
    "                        rpn_anchor_generator= anchor_generator,\n",
    "                        rpn_pre_nms_top_n_train=6000, rpn_pre_nms_top_n_test=6000,\n",
    "                        rpn_post_nms_top_n_train= 2000, rpn_post_nms_top_n_test=300,\n",
    "                        rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "                        rpn_batch_size_per_image= 256, rpn_positive_fraction=0.5,\n",
    "                        box_roi_pool=roi_pooler, box_head= box_head, box_predictor = box_predictor,\n",
    "                        box_score_thresh=0.05, box_nms_thresh=0.7, box_detections_per_img=300,\n",
    "                        box_fg_iou_thresh=0.5, box_be_iou_thresh=0.5,\n",
    "                        box_batch_size_per_image=128, box_positive_fraction=0.25\n",
    "                    )                      # model 하이퍼 파리미터 부분 : output  size 바꾸면 다 바꿔야 됨 \n",
    "for param in model.rpn.parameters():\n",
    "    torch.nn.init.normal_(param, mean=0.0, std=0.01)     # normalize\n",
    "\n",
    "for name, param in model.roi_heads.named_parameters():\n",
    "    if \"bbox_pred\" in name:\n",
    "        torch.nn.init.normal_(param, mean=0.0, std=0.001)\n",
    "    elif \"weight\" in name:\n",
    "        torch.nn.init.normal_(param, mean =0.0, std=0.01)\n",
    "    if \"bias\" in name:\n",
    "        torch.nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7db9b22-8300-4639-b368-47bc65835de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function 정\n",
    "def Total_loss(loss):\n",
    "    loss_objectness = loss['loss_objectness']\n",
    "    loss_rpn_box_reg = loss['loss_rpn_box_reg']\n",
    "    loss_classifier = loss['loss_classifier']\n",
    "    loss_box_reg = loss['loss_box_reg']\n",
    "    \n",
    "    rpn_total = loss_objectness + 10*loss_rpn_box_reg  #(람다라고 보면된다)\n",
    "    fast_rcnn_total = loss_classifier + 1*loss_box_reg\n",
    "    \n",
    "    total_loss = rpn_total + fast_rcnn_total\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c63f5-75f4-4c8a-b3d2-bf97399f7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_epoch = 0 , start_idx = 0\n",
      "Training Start\n",
      "epoch:0\n",
      "loss:tensor(3.9004, grad_fn=<AddBackward0>)\n",
      "epoch:1\n",
      "loss:tensor(3.9662, grad_fn=<AddBackward0>)\n",
      "epoch:2\n",
      "loss:tensor(3.0169, grad_fn=<AddBackward0>)\n",
      "epoch:3\n",
      "loss:tensor(2.8566, grad_fn=<AddBackward0>)\n",
      "epoch:4\n",
      "loss:tensor(3.5908, grad_fn=<AddBackward0>)\n",
      "epoch:5\n",
      "loss:tensor(2.6238, grad_fn=<AddBackward0>)\n",
      "epoch:6\n",
      "loss:tensor(3.5383, grad_fn=<AddBackward0>)\n",
      "epoch:7\n",
      "loss:tensor(3.3833, grad_fn=<AddBackward0>)\n",
      "epoch:8\n",
      "loss:tensor(2.3097, grad_fn=<AddBackward0>)\n",
      "epoch:9\n",
      "loss:tensor(2.9619, grad_fn=<AddBackward0>)\n",
      "epoch:10\n",
      "loss:tensor(2.3994, grad_fn=<AddBackward0>)\n",
      "epoch:11\n",
      "loss:tensor(2.7995, grad_fn=<AddBackward0>)\n",
      "epoch:12\n",
      "loss:tensor(2.4227, grad_fn=<AddBackward0>)\n",
      "epoch:13\n",
      "loss:tensor(2.7964, grad_fn=<AddBackward0>)\n",
      "epoch:14\n",
      "loss:tensor(3.8370, grad_fn=<AddBackward0>)\n",
      "epoch:15\n",
      "loss:tensor(3.3450, grad_fn=<AddBackward0>)\n",
      "epoch:16\n",
      "loss:tensor(3.5231, grad_fn=<AddBackward0>)\n",
      "epoch:17\n",
      "loss:tensor(2.4979, grad_fn=<AddBackward0>)\n",
      "epoch:18\n",
      "loss:tensor(1.8962, grad_fn=<AddBackward0>)\n",
      "epoch:19\n",
      "loss:tensor(2.5211, grad_fn=<AddBackward0>)\n",
      "epoch:20\n",
      "loss:tensor(3.2648, grad_fn=<AddBackward0>)\n",
      "epoch:21\n",
      "loss:tensor(2.7119, grad_fn=<AddBackward0>)\n",
      "epoch:22\n",
      "loss:tensor(2.3480, grad_fn=<AddBackward0>)\n",
      "epoch:23\n",
      "loss:tensor(2.5233, grad_fn=<AddBackward0>)\n",
      "epoch:24\n",
      "loss:tensor(2.0469, grad_fn=<AddBackward0>)\n",
      "epoch:25\n",
      "loss:tensor(2.7010, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "total_epoch = 50\n",
    "\n",
    "len_data = 10  # 한 에폭당 이미지 개수  \n",
    "\n",
    "loss_sum = 0  \n",
    "\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr=0.001, momentum=0.9, weight_decay = 0.0005) # SGD optimizer \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch,eta_min=0.00001)  # scheduler 사용 \n",
    "\n",
    "start_epoch = 0\n",
    "start_idx = 0\n",
    "\n",
    "print(\"start_epoch = {} , start_idx = {}\".format(start_epoch,start_idx))\n",
    "\n",
    "print(\"Training Start\")\n",
    "model.train()       # train 모드 \n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, total_epoch):\n",
    "    \n",
    "    dataset = Pascal_Vo(xml_list[:len_data], len_data - start_idx)   # dataset 불러오기 \n",
    "    dataloader = DataLoader(dataset, shuffle=True)    # dataloader 생성\n",
    "    \n",
    "    for i, (image, targets) in enumerate(dataloader, start_idx):   # index image target(label) 불러오기 \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        targets[0]['boxes'].squeeze_(0)\n",
    "        targets[0]['labels'].squeeze_(0)\n",
    "        \n",
    "        loss = model(image,targets)   \n",
    "        total_loss = Total_loss(loss)\n",
    "        loss_sum += total_loss\n",
    "        \n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    start_idx = 0\n",
    "    scheduler.step()\n",
    "    \n",
    "    state ={\n",
    "        'epoch' : epoch,\n",
    "        'iter' :i+1,\n",
    "        'state_dict' :model.state_dict(),\n",
    "        'optimizer':optimizer.state_dict(),\n",
    "        'scheduler' : scheduler.state_dict()\n",
    "    }\n",
    "    print('epoch:' + str(epoch))\n",
    "    print('loss:' + str(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40996e-8813-4fe5-a41a-890eaf9a085e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
