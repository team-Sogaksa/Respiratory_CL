{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aIYDdZT2GwD",
    "outputId": "52a49284-a98e-444a-8f7a-1c4cc8efd299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.50.0 kiwisolver-1.4.5 matplotlib-3.8.3 pillow-10.2.0 pyparsing-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sz9lL8VA4s-c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wav4XaYc6Q-d"
   },
   "source": [
    "### Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhJcOWzz6gjn",
    "outputId": "706158ea-a2b5-4f4b-f865-25d08c2ffb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<annotation>\n",
      "\t\n",
      "    \n",
      "\t<folder>crack</folder>\n",
      "\t\n",
      "    \n",
      "\t<filename>195_1b1_Lr_sc_Litt3200_3.png</filename>\n",
      "\t\n",
      "    \n",
      "\t<path>/content/drive/MyDrive/project/crack/195_1b1_Lr_sc_Litt3200_3.png</path>\n",
      "\t\n",
      "    \n",
      "\t<source>\n",
      "\t\t\n",
      "        \n",
      "\t\t<database>Unknown</database>\n",
      "\t\t\n",
      "    \n",
      "\t</source>\n",
      "\t\n",
      "    \n",
      "\t<size>\n",
      "\t\t\n",
      "        \n",
      "\t\t<width>224</width>\n",
      "\t\t\n",
      "        \n",
      "\t\t<height>224</height>\n",
      "\t\t\n",
      "        \n",
      "\t\t<depth>3</depth>\n",
      "\t\t\n",
      "    \n",
      "\t</size>\n",
      "\t\n",
      "    \n",
      "\t<segmented>0</segmented>\n",
      "\t\n",
      "    \n",
      "\t<object>\n",
      "\t\t\n",
      "        \n",
      "\t\t<name>crackles</name>\n",
      "\t\t\n",
      "        \n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t\n",
      "        \n",
      "\t\t<truncated>0</truncated>\n",
      "\t\t\n",
      "        \n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t\n",
      "        \n",
      "\t\t<bndbox>\n",
      "\t\t\t\n",
      "            \n",
      "\t\t\t<xmin>74</xmin>\n",
      "\t\t\t\n",
      "            \n",
      "\t\t\t<ymin>205</ymin>\n",
      "\t\t\t\n",
      "            \n",
      "\t\t\t<xmax>149</xmax>\n",
      "\t\t\t\n",
      "            \n",
      "\t\t\t<ymax>215</ymax>\n",
      "\t\t\t\n",
      "        \n",
      "\t\t</bndbox>\n",
      "\t\t\n",
      "    \n",
      "\t</object>\n",
      "\t\n",
      "\n",
      "\t<object>\n",
      "\t\t<name>wheezes</name>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>74</xmin>\n",
      "\t\t\t<ymin>115</ymin>\n",
      "\t\t\t<xmax>149</xmax>\n",
      "\t\t\t<ymax>206</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "</annotation>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a label from the annotation folder. This label is .xml format.\n",
    "with open(\"/content/drive/MyDrive/project/xml/all_xml/195_1b1_Lr_sc_Litt3200_3.xml\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUWQVIo-7GI1"
   },
   "source": [
    "### Preparing Data For Model(YoloV9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZFPG1ZFDPsk"
   },
   "source": [
    "#### Translate '.xml\" format to \".txt\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D0Q70KlJ7M1_"
   },
   "outputs": [],
   "source": [
    "# The functions are about translating label data.\n",
    "def xml_to_yolo_bbox(bbox, w, h):\n",
    "    x_center = ((bbox[2] + bbox[0]) / 2) / w\n",
    "    y_center = ((bbox[3] + bbox[1]) / 2 ) / h\n",
    "\n",
    "    width = (bbox[2] - bbox[0]) / w\n",
    "    height = (bbox[3] - bbox[1]) / h\n",
    "\n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "def yolo_to_xml_bbox(bbox, w, h):\n",
    "    # x_center, y_center, width, height\n",
    "    w_half_len = (bbox[2] + w) / 2\n",
    "    h_half_len = (bbox[3] + h) / 2\n",
    "\n",
    "    xmin = int((bbox[0] + w) - w_half_len)\n",
    "    ymin = int((bbox[1] + h) - h_half_len)\n",
    "    xmax = int((bbox[0] + w) + w_half_len)\n",
    "    ymax = int((bbox[1] + h) + h_half_len)\n",
    "\n",
    "    return [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tt8EUlCdDZvB"
   },
   "outputs": [],
   "source": [
    "# Create .txt label data.\n",
    "classes = []\n",
    "\n",
    "input_dir = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/xml_file/xml_file\"\n",
    "output_dir = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/text_file\"\n",
    "image_dir = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/mel_images/mel_images\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "files = glob.glob(os.path.join(input_dir, \"*.xml\"))\n",
    "for fil in files:\n",
    "    basename = os.path.basename(fil)\n",
    "    filename = os.path.splitext(basename)[0]\n",
    "    if not os.path.exists(os.path.join(image_dir, f\"{filename}.png\")):\n",
    "        print(f\"{filename} image does not exist\")\n",
    "        continue\n",
    "\n",
    "    result = []\n",
    "\n",
    "    tree = ET.parse(fil)\n",
    "    root = tree.getroot()\n",
    "    width = int(root.find(\"size\").find(\"width\").text)\n",
    "    height = int(root.find(\"size\").find(\"height\").text)\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        label = obj.find(\"name\").text\n",
    "\n",
    "        if label not in classes:\n",
    "            classes.append(label)\n",
    "\n",
    "        index = classes.index(label)\n",
    "        pil_bbox = [int(x.text) for x in obj.find(\"bndbox\")]\n",
    "        yolo_bbox = xml_to_yolo_bbox(pil_bbox, width, height)\n",
    "\n",
    "        bbox_string = \" \".join([str(x) for x in yolo_bbox])\n",
    "        result.append(f\"{index} {bbox_string}\")\n",
    "\n",
    "    if result:\n",
    "\n",
    "        with open(os.path.join(output_dir, f\"{filename}.txt\"), \"w\", encoding = \"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(result))\n",
    "\n",
    "with open(f\"{output_dir}/classes.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "    f.write(json.dumps(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 안에 파일 개수: 2978\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    count = 0\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "folder_path = '/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/text_file'  # 파일 개수를 확인할 폴더 경로\n",
    "file_count = count_files_in_folder(folder_path)\n",
    "print(\"폴더 안에 파일 개수:\", file_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0I0G842DbrM",
    "outputId": "97a35a3d-e8f6-4da9-ccd2-845193852af7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"crackles\", \"wheezes\", \"Normal\"]\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "with open(f\"{output_dir}/classes.txt\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_SCtcYklDcTo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<annotation>\n",
      "    <folder>107_2b4_Ll_mc_AKGC417L</folder>\n",
      "    <filename>107_2b4_Ll_mc_AKGC417L_3.png</filename>\n",
      "    <path>/content/drive/MyDrive/AIFFEL/AIFFELTON/dataset/7s_mel/crackles/image_crackle/107_2b4_Ll_mc_AKGC417L/107_2b4_Ll_mc_AKGC417L_3.png</path>\n",
      "    <source>\n",
      "        <database>Unknown</database>\n",
      "    </source>\n",
      "    <size>\n",
      "        <width>310</width>\n",
      "        <height>308</height>\n",
      "        <depth>3</depth>\n",
      "    </size>\n",
      "    <segmented>0</segmented>\n",
      "    <object>\n",
      "        <name>crackles</name>\n",
      "        <pose>Unspecified</pose>\n",
      "        <truncated>0</truncated>\n",
      "        <difficult>0</difficult>\n",
      "        <bndbox>\n",
      "            <xmin>235</xmin>\n",
      "            <ymin>256</ymin>\n",
      "            <xmax>285</xmax>\n",
      "            <ymax>295</ymax>\n",
      "        </bndbox>\n",
      "    </object>\n",
      "    <object>\n",
      "        <name>crackles</name>\n",
      "        <pose>Unspecified</pose>\n",
      "        <truncated>0</truncated>\n",
      "        <difficult>0</difficult>\n",
      "        <bndbox>\n",
      "            <xmin>108</xmin>\n",
      "            <ymin>256</ymin>\n",
      "            <xmax>231</xmax>\n",
      "            <ymax>295</ymax>\n",
      "        </bndbox>\n",
      "    </object>\n",
      "    <object>\n",
      "        <name>crackles</name>\n",
      "        <pose>Unspecified</pose>\n",
      "        <truncated>0</truncated>\n",
      "        <difficult>0</difficult>\n",
      "        <bndbox>\n",
      "            <xmin>4</xmin>\n",
      "            <ymin>256</ymin>\n",
      "            <xmax>104</xmax>\n",
      "            <ymax>295</ymax>\n",
      "        </bndbox>\n",
      "    </object>\n",
      "</annotation>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .xml format\n",
    "with open(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/xml_file/xml_file/107_2b4_Ll_mc_AKGC417L_3.xml\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PbYMTv4PDeqv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9193548387096774 0.6672077922077922 0.15483870967741936 0.5811688311688312\n",
      "0 0.6983870967741935 0.6672077922077922 0.26129032258064516 0.5811688311688312\n",
      "0 0.45806451612903226 0.6672077922077922 0.1935483870967742 0.5811688311688312\n",
      "0 0.22096774193548388 0.6672077922077922 0.25483870967741934 0.5811688311688312\n",
      "0 0.04838709677419355 0.6672077922077922 0.06451612903225806 0.5811688311688312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .txt format\n",
    "with open(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/augmentation/101_1b1_Al_sc_Meditron_1_aug.txt\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glcwKvIGDhWy",
    "outputId": "7b5513ab-4aa5-4dcd-a81a-3ca64f4e3219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation Counts: 2989\n",
      "Lables Counts: 2979\n"
     ]
    }
   ],
   "source": [
    "annotation_count = len(os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/xml_file/xml_file\"))\n",
    "labels_count = len(os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/text_file\"))\n",
    "print(f\"Annotation Counts: {annotation_count}\")\n",
    "print(f\"Lables Counts: {labels_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NM2RCkzUWnaY"
   },
   "outputs": [],
   "source": [
    "annotation_list = os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/xml_file/xml_file\")\n",
    "labels_list = os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/text_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az7lkGQDEfei"
   },
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EKgBKbpeEcaU"
   },
   "outputs": [],
   "source": [
    "# Create a folder of train dataset\n",
    "if not os.path.isfile(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/data\"):\n",
    "    # os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/images')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/labels')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/images')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/labels')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/images')\n",
    "    os.mkdir('/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6dtnyKFUE4i6"
   },
   "outputs": [],
   "source": [
    "# Material is the names of images in the images folder.\n",
    "metarial = []\n",
    "for i in os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/mel_images/mel_images\"):\n",
    "    str1 = i[:-4]\n",
    "    metarial.append(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbnYtAv5lgn5",
    "outputId": "55e84656-e7fb-4bc1-fb9f-ec2373f38b23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['118_1b1_Pr_sc_Litt3200_2',\n",
       " '138_1p2_Pr_mc_AKGC417L_3',\n",
       " '167_1b1_Al_sc_Meditron_2',\n",
       " '163_2b2_Ar_mc_AKGC417L_2',\n",
       " '151_2p4_Lr_mc_AKGC417L_2',\n",
       " '172_1b3_Ar_mc_AKGC417L_3',\n",
       " '223_1b1_Ar_sc_Meditron_2',\n",
       " '133_2p2_Ar_mc_AKGC417L_2',\n",
       " '107_2b3_Ar_mc_AKGC417L_3',\n",
       " '205_1b3_Lr_mc_AKGC417L_1',\n",
       " '198_1b5_Al_mc_AKGC417L_1',\n",
       " '200_2p3_Pl_mc_AKGC417L_3',\n",
       " '107_3p2_Pl_mc_AKGC417L_2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metarial[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6pz8p6Wlm-p",
    "outputId": "3d6eb788-5352-43e3-87c5-39a3522d9e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of image:  2992\n",
      "Sum of the train size:  2094\n",
      "Sum of the test size:  448\n",
      "Sum of the val size:  448\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of image: \", len(metarial))\n",
    "train_size = int(len(metarial) * 0.7)\n",
    "test_size = int(len(metarial) * 0.15)\n",
    "val_size = int(len(metarial) * 0.15)\n",
    "print(\"Sum of the train size: \", train_size)\n",
    "print(\"Sum of the test size: \", test_size)\n",
    "print(\"Sum of the val size: \", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "k1juw7WaluXV"
   },
   "outputs": [],
   "source": [
    "def preparingdata(main_txt_file, main_img_file, train_size, test_size, val_size):\n",
    "    metarial = sorted([os.path.splitext(f)[0] for f in os.listdir(main_img_file) if f.endswith(\".png\")])\n",
    "    \n",
    "    available_files = []\n",
    "    for m in metarial:\n",
    "        img_file = os.path.join(main_img_file, m + \".png\")\n",
    "        txt_file = os.path.join(main_txt_file, m + \".txt\")\n",
    "        if os.path.exists(img_file) and os.path.exists(txt_file):\n",
    "            available_files.append(m)\n",
    "    \n",
    "    total_files = len(available_files)\n",
    "    \n",
    "    if total_files < train_size + test_size + val_size:\n",
    "        print(\"Warning: Insufficient data for specified train, test, and validation sizes.\")\n",
    "        print(\"Total available files:\", total_files)\n",
    "        print(\"Adjusting train, test, and validation sizes.\")\n",
    "        train_size = int(0.6 * total_files)  # 60% for training\n",
    "        test_size = int(0.2 * total_files)   # 20% for testing\n",
    "        val_size = total_files - train_size - test_size  # Remaining for validation\n",
    "    \n",
    "    train_files = available_files[:train_size]\n",
    "    test_files = available_files[train_size:train_size+test_size]\n",
    "    val_files = available_files[train_size+test_size:]\n",
    "    \n",
    "    # Copy files for training\n",
    "    for i in range(train_size):\n",
    "        source_txt = os.path.join(main_txt_file, train_files[i] + \".txt\")\n",
    "        source_img = os.path.join(main_img_file, train_files[i] + \".png\")\n",
    "        \n",
    "        train_destination_txt = os.path.join(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/labels\", train_files[i] + \".txt\")\n",
    "        train_destination_png = os.path.join(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/images\", train_files[i] + \".png\")\n",
    "\n",
    "        shutil.copy(source_txt, train_destination_txt)\n",
    "        shutil.copy(source_img, train_destination_png)\n",
    "\n",
    "    # Copy files for testing\n",
    "    for l in range(test_size):\n",
    "        source_txt = os.path.join(main_txt_file, test_files[l] + \".txt\")\n",
    "        source_img = os.path.join(main_img_file, test_files[l] + \".png\")\n",
    "        \n",
    "        test_destination_txt = os.path.join(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/labels\", test_files[l] + \".txt\")\n",
    "        test_destination_png = os.path.join(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/images\", test_files[l] + \".png\")\n",
    "\n",
    "        shutil.copy(source_txt, test_destination_txt)\n",
    "        shutil.copy(source_img, test_destination_png)\n",
    "\n",
    "    # Copy files for validation\n",
    "    for n in range(val_size):\n",
    "        source_txt = os.path.join(main_txt_file, val_files[n] + \".txt\")\n",
    "        source_img = os.path.join(main_img_file, val_files[n] + \".png\")\n",
    "        \n",
    "        val_destination_txt = os.path.join(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/labels\", val_files[n] + \".txt\")\n",
    "        val_destination_png = os.path.join(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/images\", val_files[n] + \".png\")\n",
    "\n",
    "        # Remove _mel_spec from the filename\n",
    "        new_filename = val_files[n]\n",
    "\n",
    "        # Copy files with modified filename\n",
    "        shutil.copy(source_txt, val_destination_txt)\n",
    "        shutil.copy(source_img, val_destination_png.replace(val_files[n], new_filename))\n",
    "\n",
    "    print(\"Data preparation completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Insufficient data for specified train, test, and validation sizes.\n",
      "Total available files: 2977\n",
      "Adjusting train, test, and validation sizes.\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "#preparinbdata(\"/kaggle/working/labels\", \"/kaggle/input/face-mask-detection/images\", 603, 150, 100)\n",
    "preparingdata(main_txt_file = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/text_file\",\n",
    "              main_img_file = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/mel_images/mel_images\",\n",
    "              train_size = train_size,\n",
    "              test_size = test_size,\n",
    "              val_size = val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OTRKmDaT33Y",
    "outputId": "b76dfa17-72e8-45fe-b3b6-ea302d439dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Insufficient data for specified train, test, and validation sizes.\n",
      "Total available files: 2977\n",
      "Adjusting train, test, and validation sizes.\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "#preparinbdata(\"/kaggle/working/labels\", \"/kaggle/input/face-mask-detection/images\", 603, 150, 100)\n",
    "preparingdata(main_txt_file = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/augmentation_file/text\",\n",
    "              main_img_file = \"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/augmentation_file/image\",\n",
    "              train_size = train_size,\n",
    "              test_size = test_size,\n",
    "              val_size = val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OTRKmDaT33Y",
    "outputId": "b76dfa17-72e8-45fe-b3b6-ea302d439dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 3869\n",
      "vol_data: 1192\n",
      "test_data: 1340\n"
     ]
    }
   ],
   "source": [
    "train_data = len(os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/images\"))\n",
    "vol_data = len(os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/images\"))\n",
    "test_data = len(os.listdir(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/images\"))\n",
    "\n",
    "print(f\"train_data: {train_data}\")\n",
    "print(f\"vol_data: {vol_data}\")\n",
    "print(f\"test_data: {test_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh5PsrA7ck_2"
   },
   "source": [
    "### yaml 파일 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "n-0g918Z_nwV"
   },
   "outputs": [],
   "source": [
    "# Create the data.yaml. I am going to use it on the train with yolov9.\n",
    "yaml_text = \"\"\"train: /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/images\n",
    "val: /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/images\n",
    "\n",
    "nc: 3\n",
    "names: [\"crackles\", \"wheezes\", \"Normal\"]\"\"\"\n",
    "\n",
    "with open(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/data.yaml\", 'w') as file:\n",
    "    file.write(yaml_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7liaV3Y6AV06",
    "outputId": "598e643b-6e59-4302-84d0-9c294c9ed582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/images\n",
      "val: /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/images\n",
      "\n",
      "nc: 3\n",
      "names: [\"crackles\", \"wheezes\", \"Normal\"]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/data.yaml\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GM3WTgM-AhNy"
   },
   "source": [
    "### YoloV9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vlce66DnAdUT",
    "outputId": "abad9f82-2ec9-455f-ed6b-ca1624712d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov9' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Download Github rep.\n",
    "!git clone https://github.com/SkalskiP/yolov9.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sogaksa123/AIFFEL_THON/model/Yolov9_ver2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTTNINAXEymy",
    "outputId": "bdb6ac51-f5c6-4021-bedf-9c78dc599f19"
   },
   "outputs": [],
   "source": [
    "# İnstall the req.\n",
    "!pip install -r /home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvsvVbh2E3D2",
    "outputId": "2efb68a7-7bbd-4e1e-9d95-bde91ebead33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-01 02:59:21.921563: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-01 02:59:22.995011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-01 02:59:24.541798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=, cfg=/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/models/detect/yolov9-c.yaml, data=/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/data.yaml, hyp=/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/data/hyps/hyp.scratch-high.yaml, epochs=50, batch_size=32, imgsz=400, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=../Yolov9/yolov9/runs/train, name=yolov9-c, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLOv5 🚀 1e33dbb Python-3.10.13 torch-2.2.1+cu121 CUDA:0 (NVIDIA L4, 22491MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, dfl=1.5, obj_pw=1.0, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir ../Yolov9/yolov9/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1         0  models.common.Silence                   []                            \n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  8                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      " 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
      " 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
      " 17                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
      " 20                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \n",
      " 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \n",
      " 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \n",
      " 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      " 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      " 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      " 29                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \n",
      " 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      " 32                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \n",
      " 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      " 35                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \n",
      " 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      " 38[31, 34, 37, 16, 19, 22]  1  21547442  models.yolo.DualDDetect                 [3, [512, 512, 512, 256, 512, 512]]\n",
      "yolov9-c summary: 962 layers, 51004210 parameters, 51004178 gradients, 238.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "WARNING ⚠️ --img-size 400 must be multiple of max stride 32, updating to 416\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 238 weight(decay=0.0), 255 weight(decay=0.0005), 253 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/tra\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/l\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/val/labels.cache\n",
      "Plotting labels to ../Yolov9/yolov9/runs/train/yolov9-c14/labels.jpg... \n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m../Yolov9/yolov9/runs/train/yolov9-c14\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/49      15.6G      4.422      5.214      5.264        215        416:  Exception in thread Thread-11 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "WARNING ⚠️ TensorBoard graph visualization failure Sizes of tensors must match except in dimension 1. Expected size 24 but got size 25 for tensor number 1 in the list.\n",
      "       0/49      15.6G      4.747      5.224      5.283        254        416:  Exception in thread Thread-12 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "       0/49      15.6G      4.821      5.217      5.286        187        416:  Exception in thread Thread-13 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "       0/49      15.7G       4.82      5.104      5.247        191        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736    0.00431       0.42    0.00956    0.00298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/49      17.2G      4.586      4.363      4.859        190        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736     0.0131     0.0351    0.00509    0.00131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/49      17.2G      3.731      3.677      4.033        224        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736     0.0507      0.134     0.0388     0.0122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/49      17.2G      2.975      3.225      3.265        139        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.431       0.22     0.0952     0.0354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/49      17.2G      2.565      2.991      2.805        224        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.475      0.332      0.146     0.0622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/49      17.6G      2.371      2.871      2.623        206        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.171      0.324      0.182     0.0905\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/49      17.6G      2.242      2.785      2.487        264        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.162      0.403      0.161       0.08\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/49      17.6G       2.15      2.743       2.42        184        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.192      0.459      0.184     0.0919\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/49      17.6G      2.048      2.672      2.333        202        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.191      0.414      0.192     0.0979\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/49      17.6G      2.008      2.624      2.291        212        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.175      0.464      0.184     0.0981\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/49      17.6G      1.951      2.575      2.234        179        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736       0.19      0.576      0.212      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/49      17.6G      1.911      2.541      2.215        200        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736       0.23      0.495      0.228      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/49      17.6G      1.869      2.499      2.168        217        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.216      0.549      0.226      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/49      17.6G      1.833      2.475      2.137        149        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.213      0.472      0.236      0.136\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/49      17.6G      1.776      2.436      2.101        183        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.219      0.471       0.22      0.117\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/49      17.6G      1.751      2.412      2.081        208        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.224      0.493      0.233       0.13\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/49      17.6G      1.735      2.389      2.062        197        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736       0.21      0.492      0.211      0.115\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/49      17.6G      1.685      2.339      2.015        150        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.228       0.51      0.228      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/49      17.6G      1.662      2.329      2.004        192        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736       0.22      0.394      0.241      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/49      17.6G      1.655      2.313      1.997        277        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.207      0.513      0.242      0.143\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/49      17.6G      1.616       2.28       1.96        196        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.253      0.534       0.26      0.169\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/49      17.6G      1.589      2.265      1.952        198        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.281      0.519      0.284      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/49      17.6G      1.567      2.243      1.925        204        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.218      0.486      0.235       0.13\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/49      17.6G      1.534      2.202      1.903        215        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.284      0.518      0.297      0.197\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/49      17.6G       1.52        2.2      1.898        180        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.252      0.519      0.261      0.168\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/49      17.6G      1.483      2.163      1.864        232        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.251      0.454      0.241      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/49      17.6G      1.487       2.16      1.857        179        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.276      0.499      0.283      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/49      17.6G      1.444      2.136      1.831        213        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.305      0.487      0.301      0.209\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/49      17.6G      1.425      2.121      1.812        188        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.254      0.534      0.263      0.173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/49      17.6G      1.415      2.093      1.807        205        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.295      0.498      0.295      0.209\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/49      17.6G      1.411      2.097      1.805        192        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.271      0.488      0.261      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/49      17.6G      1.395      2.074      1.787        182        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.288      0.496      0.287      0.191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/49      17.6G      1.391      2.082       1.78        214        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.303      0.527       0.31      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/49      17.6G      1.359      2.048      1.765        211        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.287      0.509      0.279      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/49      17.6G      1.363      2.052      1.766        215        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.279      0.517      0.295      0.197\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/49      17.6G       1.21      2.041      1.741         75        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.276      0.571      0.286      0.194\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/49      17.6G      1.164      1.982      1.687         73        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.281      0.528      0.282      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/49      17.6G      1.132      1.963      1.674         82        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.325      0.518      0.313      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/49      17.6G      1.105       1.93       1.66         84        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.305      0.511      0.296      0.201\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/49      17.6G      1.085      1.903      1.642         77        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.255      0.553      0.257      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/49      17.6G      1.061      1.862      1.615         88        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.301      0.528      0.304      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/49      17.6G      1.066      1.867      1.628         76        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.284      0.539      0.298      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/49      17.6G      1.036      1.836      1.602         84        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.306      0.545      0.302      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/49      17.6G       1.03      1.816      1.588         81        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.302      0.584      0.311      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/49      17.6G      1.016      1.787      1.576         75        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736        0.3      0.548       0.31      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/49      17.6G     0.9873      1.772      1.566         64        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736       0.28       0.51      0.285      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/49      17.6G     0.9767      1.757      1.558         82        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.311      0.539      0.307       0.22\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/49      17.6G     0.9794      1.723      1.555         88        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.311      0.571      0.325      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/49      17.6G      0.951      1.699      1.531         91        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.302       0.59      0.306      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/49      17.6G      0.946      1.692      1.545         85        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.309      0.585      0.323      0.238\n",
      "\n",
      "50 epochs completed in 1.638 hours.\n",
      "Optimizer stripped from ../Yolov9/yolov9/runs/train/yolov9-c14/weights/last.pt, 102.8MB\n",
      "Optimizer stripped from ../Yolov9/yolov9/runs/train/yolov9-c14/weights/best.pt, 102.8MB\n",
      "\n",
      "Validating ../Yolov9/yolov9/runs/train/yolov9-c14/weights/best.pt...\n",
      "Fusing layers... \n",
      "yolov9-c summary: 724 layers, 50963250 parameters, 0 gradients, 237.7 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   Exception in thread Thread-63 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "Exception in thread Thread-64 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "                 Class     Images  Instances          P          R      mAP50   Exception in thread Thread-66 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "Exception in thread Thread-65 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "                 Class     Images  Instances          P          R      mAP50   Exception in thread Thread-68 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "Exception in thread Thread-67 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sogaksa123/anaconda3/envs/sogaksa/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1192       3736      0.312      0.571      0.325      0.243\n",
      "              crackles       1192       1544      0.363       0.63      0.378      0.283\n",
      "               wheezes       1192        841      0.231      0.451      0.237       0.18\n",
      "                Normal       1192       1351      0.341      0.634       0.36      0.266\n",
      "Results saved to \u001b[1m../Yolov9/yolov9/runs/train/yolov9-c14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train with YoloV9\n",
    "\"\"\"\n",
    "workers --> max dataloader workers (per RANK in DDP mode)\n",
    "device --> cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "batch --> total batch size for all GPUs, -1 for autobatch\n",
    "epochs --> total training epochs\n",
    "data --> dataset.yaml path\n",
    "img --> train, val image size (pixels)\n",
    "cfg --> model.yaml path\n",
    "weights --> initial weights path\n",
    "name --> save to project/name\n",
    "hyp --> hyperparameters path\n",
    "\n",
    "**This parameters is my project parameters. You can make changes for your project.**\n",
    "\"\"\"\n",
    "\n",
    "!python /home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/train_dual.py \\\n",
    "--workers 8 --device 0  --batch 32 --epochs 50 --min-items 0 --close-mosaic 15\\\n",
    "--data /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/data.yaml \\\n",
    "--img 400 --cfg  /home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/models/detect/yolov9-c.yaml \\\n",
    "--weights '' --name yolov9-c --hyp /home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/data/hyps/hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iCLKgX5I60x"
   },
   "source": [
    "##\n",
    "Class: 클래스 레이블입니다.  \n",
    "Images: 해당 클래스에 대한 이미지 수입니다.  \n",
    "Instances: 해당 클래스에 대한 감지된 객체의 총 수입니다.  \n",
    "P: 정밀도(Precision)는 모델이 예측한 객체 중 실제로 해당 클래스에 속하는 객체의 비율입니다. 높은 정밀도는 모델이 정확하게 예측한 것을 의미합니다.  \n",
    "R: 재현율(Recall)은 실제 해당 클래스에 속하는 객체 중 모델이 정확하게 감지한 객체의 비율입니다. 높은 재현율은 모델이 대상을 놓치지 않고 잘 감지한다는 것을 의미합니다.  \n",
    "mAP50: 평균 정밀도(mean Average Precision)는 다양한 IoU(Intersection over Union) 임계값에 대해 계산된 정밀도의 평균입니다. 이 경우, IoU 임계값이 0.5인 경우의 mAP입니다. 높은 mAP는 모델의 성능이 좋다는 것을 의미합니다.\n",
    "\n",
    "해석:  \n",
    "전체적으로 모델의 성능은 중간 정도로 보입니다. 정밀도와 재현율이 비슷한 수준이며, 평균 정밀도도 0.3 정도로 나쁘지 않은 편입니다.\n",
    "클래스 별로 살펴보면, crackles와 wheezes 클래스의 재현율이 상대적으로 높지만 정밀도는 낮습니다. 이는 모델이 해당 클래스의 객체를 많이 감지하지만 그 중에서 실제 해당 클래스에 속하는 객체의 비율이 낮다는 것을 의미합니다.\n",
    "개선을 위해서는 모델의 학습 데이터의 다양성을 높이거나, 모델 아키텍처를 조정하여 성능을 향상시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iCLKgX5I60x"
   },
   "source": [
    "##\n",
    "Class: 클래스 레이블입니다.  \n",
    "Images: 해당 클래스에 대한 이미지 수입니다.  \n",
    "Instances: 해당 클래스에 대한 감지된 객체의 총 수입니다.  \n",
    "P: 정밀도(Precision)는 모델이 예측한 객체 중 실제로 해당 클래스에 속하는 객체의 비율입니다. 높은 정밀도는 모델이 정확하게 예측한 것을 의미합니다.  \n",
    "R: 재현율(Recall)은 실제 해당 클래스에 속하는 객체 중 모델이 정확하게 감지한 객체의 비율입니다. 높은 재현율은 모델이 대상을 놓치지 않고 잘 감지한다는 것을 의미합니다.  \n",
    "mAP50: 평균 정밀도(mean Average Precision)는 다양한 IoU(Intersection over Union) 임계값에 대해 계산된 정밀도의 평균입니다. 이 경우, IoU 임계값이 0.5인 경우의 mAP입니다. 높은 mAP는 모델의 성능이 좋다는 것을 의미합니다.\n",
    "\n",
    "해석:  \n",
    "전체적으로 모델의 성능은 중간 정도로 보입니다. 정밀도와 재현율이 비슷한 수준이며, 평균 정밀도도 0.3 정도로 나쁘지 않은 편입니다.\n",
    "클래스 별로 살펴보면, crackles와 wheezes 클래스의 재현율이 상대적으로 높지만 정밀도는 낮습니다. 이는 모델이 해당 클래스의 객체를 많이 감지하지만 그 중에서 실제 해당 클래스에 속하는 객체의 비율이 낮다는 것을 의미합니다.\n",
    "개선을 위해서는 모델의 학습 데이터의 다양성을 높이거나, 모델 아키텍처를 조정하여 성능을 향상시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# YOLOv5 모델 로드\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# 이미지 로드\n",
    "image_path = 'your_image.jpg'  # 이미지 경로\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 객체 감지\n",
    "results = model(image)\n",
    "\n",
    "# 결과 시각화\n",
    "results.show()\n",
    "\n",
    "# 결과를 이미지로 저장\n",
    "results.save('result.jpg')\n",
    "\n",
    "# 결과를 numpy 배열로 반환\n",
    "result_image = results.render()[0]\n",
    "result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('result_cv2.jpg', result_image)\n",
    "\n",
    "# 결과를 matplotlib으로 시각화\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AIFFEL_THON' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mAIFFEL_THON\u001b[49m\u001b[38;5;241m/\u001b[39mmodel\u001b[38;5;241m/\u001b[39mYolov9\u001b[38;5;241m/\u001b[39myolov9\u001b[38;5;241m/\u001b[39mdetect\u001b[38;5;241m.\u001b[39mpy\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AIFFEL_THON' is not defined"
     ]
    }
   ],
   "source": [
    "AIFFEL_THON/model/Yolov9/yolov9/detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3029363345.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    python val.py --data data/coco.yaml --img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 --weights './yolov9-c-converted.pt' --save-json --name yolov9_c_c_640_val\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# evaluate converted yolov9 models\n",
    "python val.py --data data/coco.yaml --img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 --weights './yolov9-c-converted.pt' --save-json --name yolov9_c_c_640_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/runs/train/yolov9-c5/weights/best.pt'], source=/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/images/186_2b3_Ar_mc_AKGC417L_3.png, data=data/coco128.yaml, imgsz=[224, 224], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=yolov9_c_c_640_detect, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 🚀 1e33dbb Python-3.10.13 torch-2.2.1+cu121 CUDA:0 (NVIDIA L4, 22491MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9-c summary: 724 layers, 50960940 parameters, 0 gradients, 237.7 GFLOPs\n",
      "image 1/1 /home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/images/186_2b3_Ar_mc_AKGC417L_3.png: 224x224 1 wheezes, 26.9ms\n",
      "Speed: 0.3ms pre-process, 26.9ms inference, 534.5ms NMS per image at shape (1, 3, 224, 224)\n",
      "Results saved to \u001b[1mruns/detect/yolov9_c_c_640_detect5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source '/home/sogaksa123/AIFFEL_THON/data/dataset2/data_rec/Training/test/images/186_2b3_Ar_mc_AKGC417L_3.png' --img 224 --device 0 --weights '/home/sogaksa123/AIFFEL_THON/model/Yolov9/yolov9/runs/train/yolov9-c5/weights/best.pt' --name yolov9_c_c_640_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
