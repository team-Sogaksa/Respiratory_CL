{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aIYDdZT2GwD",
        "outputId": "31614feb-07d8-4a9d-c967-803569dca7a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz9lL8VA4s-c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import xml.etree.cElementTree as ET\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from PIL import Image, ImageOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wav4XaYc6Q-d"
      },
      "source": [
        "###Review Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uhJcOWzz6gjn",
        "outputId": "6b6969e0-5313-4134-b4ff-c3bafda7a14c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<?xml version=\"1.0\" ?>\n",
            "<annotation>\n",
            "    <folder>crackles_image</folder>\n",
            "    <filename>128_1b3_Tc_mc_LittC2SE_0_s.png</filename>\n",
            "    <path>/content/drive/MyDrive/AIFFEL/AIFFELTON/label_data/crackles_image/128_1b3_Tc_mc_LittC2SE_0_crackles.png</path>\n",
            "    <source>\n",
            "        <database>Unknown</database>\n",
            "    </source>\n",
            "    <size>\n",
            "        <width>969</width>\n",
            "        <height>370</height>\n",
            "        <depth>3</depth>\n",
            "    </size>\n",
            "    <segmented>0</segmented>\n",
            "    <object>\n",
            "        <name>crackles</name>\n",
            "        <pose>Unspecified</pose>\n",
            "        <truncated>0</truncated>\n",
            "        <difficult>0</difficult>\n",
            "        <bndbox>\n",
            "            <xmin>261</xmin>\n",
            "            <ymin>258</ymin>\n",
            "            <xmax>705</xmax>\n",
            "            <ymax>352</ymax>\n",
            "        </bndbox>\n",
            "    </object>\n",
            "</annotation>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This is a label from the annotation folder. This label is .xml format.\n",
        "with open(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/label_data/co_dataset/xml_file/xml_file/128_1b3_Tc_mc_LittC2SE_0.xml\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUWQVIo-7GI1"
      },
      "source": [
        "###Preparing Data For Model(YoloV9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZFPG1ZFDPsk"
      },
      "source": [
        "####Translate '.xml\" format to \".txt\" format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Q70KlJ7M1_"
      },
      "outputs": [],
      "source": [
        "# The functions are about translating label data.\n",
        "def xml_to_yolo_bbox(bbox, w, h):\n",
        "    x_center = ((bbox[2] + bbox[0]) / 2) / w\n",
        "    y_center = ((bbox[3] + bbox[1]) / 2 ) / h\n",
        "\n",
        "    width = (bbox[2] - bbox[0]) / w\n",
        "    height = (bbox[3] - bbox[1]) / h\n",
        "\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "def yolo_to_xml_bbox(bbox, w, h):\n",
        "    # x_center, y_center, width, height\n",
        "    w_half_len = (bbox[2] + w) / 2\n",
        "    h_half_len = (bbox[3] + h) / 2\n",
        "\n",
        "    xmin = int((bbox[0] + w) - w_half_len)\n",
        "    ymin = int((bbox[1] + h) - h_half_len)\n",
        "    xmax = int((bbox[0] + w) + w_half_len)\n",
        "    ymax = int((bbox[1] + h) + h_half_len)\n",
        "\n",
        "    return [xmin, ymin, xmax, ymax]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt8EUlCdDZvB"
      },
      "outputs": [],
      "source": [
        "# Create .txt label data.\n",
        "classes = []\n",
        "\n",
        "input_dir = \"/kaggle/input/face-mask-detection/annotations\"\n",
        "output_dir = \"/kaggle/working/labels\"\n",
        "image_dir = \"/kaggle/input/face-mask-detection/images\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "files = glob.glob(os.path.join(input_dir, \"*.xml\"))\n",
        "for fil in files:\n",
        "    basename = os.path.basename(fil)\n",
        "    filename = os.path.splitext(basename)[0]\n",
        "    if not os.path.exists(os.path.join(image_dir, f\"{filename}.png\")):\n",
        "        print(f\"{filename} image does not exist\")\n",
        "        continue\n",
        "\n",
        "    result = []\n",
        "\n",
        "    tree = ET.parse(fil)\n",
        "    root = tree.getroot()\n",
        "    width = int(root.find(\"size\").find(\"width\").text)\n",
        "    height = int(root.find(\"size\").find(\"height\").text)\n",
        "\n",
        "    for obj in root.findall(\"object\"):\n",
        "        label = obj.find(\"name\").text\n",
        "\n",
        "        if label not in classes:\n",
        "            classes.append(label)\n",
        "\n",
        "        index = classes.index(label)\n",
        "        pil_bbox = [int(x.text) for x in obj.find(\"bndbox\")]\n",
        "        yolo_bbox = xml_to_yolo_bbox(pil_bbox, width, height)\n",
        "\n",
        "        bbox_string = \" \".join([str(x) for x in yolo_bbox])\n",
        "        result.append(f\"{index} {bbox_string}\")\n",
        "\n",
        "    if result:\n",
        "\n",
        "        with open(os.path.join(output_dir, f\"{filename}.txt\"), \"w\", encoding = \"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(result))\n",
        "\n",
        "with open(f\"{output_dir}/classes.txt\", \"w\", encoding = \"utf-8\") as f:\n",
        "    f.write(json.dumps(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0I0G842DbrM"
      },
      "outputs": [],
      "source": [
        "# Labels\n",
        "with open(f\"{output_dir}/classes.txt\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SCtcYklDcTo"
      },
      "outputs": [],
      "source": [
        "# .xml format\n",
        "with open(\"/kaggle/input/face-mask-detection/annotations/maksssksksss334.xml\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYMTv4PDeqv"
      },
      "outputs": [],
      "source": [
        "# .txt format\n",
        "with open(\"/kaggle/working/labels/maksssksksss334.txt\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glcwKvIGDhWy",
        "outputId": "26365883-3989-4587-a5f1-48a4b06958dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotation Counts: 6898\n",
            "Lables Counts: 6898\n"
          ]
        }
      ],
      "source": [
        "annotation_count = len(os.listdir(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/label_data/co_dataset/xml_file/xml_file\"))\n",
        "labels_count = len(os.listdir(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV7/labels\"))\n",
        "print(f\"Annotation Counts: {annotation_count}\")\n",
        "print(f\"Lables Counts: {labels_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az7lkGQDEfei"
      },
      "source": [
        "###Preparing Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EKgBKbpeEcaU"
      },
      "outputs": [],
      "source": [
        "# Create a folder of train dataset\n",
        "if not os.path.isfile(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data\"):\n",
        "    # os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/test')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/images')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/labels')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/test/images')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/test/labels')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/images')\n",
        "    os.mkdir('/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6dtnyKFUE4i6"
      },
      "outputs": [],
      "source": [
        "# Material is the names of images in the images folder.\n",
        "metarial = []\n",
        "for i in os.listdir(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/label_data/co_dataset/mel_image\"):\n",
        "    str = i[:-13]\n",
        "    metarial.append(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbnYtAv5lgn5",
        "outputId": "68176159-9e6f-4236-e7a5-1b26e04856a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['192_2b3_Ar_mc_LittC2SE_1',\n",
              " '192_2b3_Ar_mc_LittC2SE_2',\n",
              " '192_2b3_Ar_mc_LittC2SE_3',\n",
              " '192_2b3_Ar_mc_LittC2SE_4',\n",
              " '192_2b3_Ar_mc_LittC2SE_5',\n",
              " '192_2b3_Ar_mc_LittC2SE_6',\n",
              " '193_1b2_Al_mc_AKGC417L_0',\n",
              " '193_1b2_Al_mc_AKGC417L_1',\n",
              " '193_1b2_Al_mc_AKGC417L_2',\n",
              " '193_1b2_Al_mc_AKGC417L_3']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "metarial[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6pz8p6Wlm-p",
        "outputId": "68ab6446-f980-4db8-ebd8-f864b940d275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of image:  6898\n",
            "Sum of the train size:  4828\n",
            "Sum of the test size:  1034\n",
            "Sum of the val size:  1034\n"
          ]
        }
      ],
      "source": [
        "print(\"Sum of image: \", len(metarial))\n",
        "train_size = int(len(metarial) * 0.7)\n",
        "test_size = int(len(metarial) * 0.15)\n",
        "val_size = int(len(metarial) * 0.15)\n",
        "print(\"Sum of the train size: \", train_size)\n",
        "print(\"Sum of the test size: \", test_size)\n",
        "print(\"Sum of the val size: \", val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k1juw7WaluXV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def preparingdata(main_txt_file, main_img_file, train_size, test_size, val_size):\n",
        "    for i in range(0, train_size):\n",
        "        source_txt = main_txt_file + \"/\" + metarial[i] + \".txt\"\n",
        "        source_img = main_img_file + \"/\" + metarial[i] + \"_mel_spec.png\"\n",
        "\n",
        "        mstring = metarial[i]\n",
        "        train_destination_txt = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/labels\" + \"/\" + metarial[i] + \".txt\"\n",
        "        train_destination_png = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/images\" + \"/\" + metarial[i] + \"_mel_spec.png\"\n",
        "\n",
        "        shutil.copy(source_txt, train_destination_txt)\n",
        "        shutil.copy(source_img, train_destination_png)\n",
        "\n",
        "    for l in range(train_size , train_size + test_size):\n",
        "        source_txt = main_txt_file + \"/\" + metarial[l] + \".txt\"\n",
        "        source_img = main_img_file + \"/\" + metarial[l] + \"_mel_spec.png\"\n",
        "\n",
        "        mstring = metarial[l]\n",
        "        test_destination_txt = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/test/labels\" + \"/\" + metarial[l] + \".txt\"\n",
        "        test_destination_png = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/test/images\" + \"/\" + metarial[l] + \"_mel_spec.png\"\n",
        "\n",
        "        shutil.copy(source_txt, test_destination_txt)\n",
        "        shutil.copy(source_img, test_destination_png)\n",
        "\n",
        "    for n in range(train_size + test_size , train_size + test_size + val_size):\n",
        "        source_txt = main_txt_file + \"/\" + metarial[n] + \".txt\"\n",
        "        source_img = main_img_file + \"/\" + metarial[n] + \"_mel_spec.png\"\n",
        "\n",
        "        mstring = metarial[n]\n",
        "        val_destination_txt = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/labels\" + \"/\" + metarial[n] + \".txt\"\n",
        "        val_destination_png = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/images\" + \"/\" + metarial[n] + \"_mel_spec.png\"\n",
        "\n",
        "        # Remove _mel_spec from the filename\n",
        "        new_filename = metarial[n].replace(\"_mel_spec\", \"\")\n",
        "\n",
        "        # Copy files with modified filename\n",
        "        shutil.copy(source_txt, val_destination_txt)\n",
        "        shutil.copy(source_img, val_destination_png.replace(metarial[n] + \"_mel_spec\", new_filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sZqOS6Gatayn"
      },
      "outputs": [],
      "source": [
        "#preparinbdata(\"/kaggle/working/labels\", \"/kaggle/input/face-mask-detection/images\", 603, 150, 100)\n",
        "preparingdata(main_txt_file = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV7/labels\",\n",
        "              main_img_file = \"/content/drive/MyDrive/AIFFEL/AIFFELTON/label_data/co_dataset/mel_image\",\n",
        "              train_size = 4828,\n",
        "              test_size = 1034,\n",
        "              val_size = 1034)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = len(os.listdir(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/images\"))\n",
        "vol_data = len(os.listdir(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/images\"))\n",
        "test_data = len(os.listdir(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/test/images\"))\n",
        "\n",
        "print(f\"train_data: {train_data}\")\n",
        "print(f\"vol_data: {vol_data}\")\n",
        "print(f\"test_data: {test_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OTRKmDaT33Y",
        "outputId": "e072bbbb-0a53-4669-fbc2-9d0cf74617f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data: 4828\n",
            "vol_data: 1034\n",
            "test_data: 1034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###yaml íŒŒì¼ ì œì‘"
      ],
      "metadata": {
        "id": "Yh5PsrA7ck_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n-0g918Z_nwV"
      },
      "outputs": [],
      "source": [
        "# Create the data.yaml. I am going to use it on the train with yolov9.\n",
        "yaml_text = \"\"\"train: /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/images/\n",
        "val: /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/images\n",
        "\n",
        "nc: 3\n",
        "names: [\"Normal\", \"crackles\", \"wheezes\"]\"\"\"\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/data.yaml\", 'w') as file:\n",
        "    file.write(yaml_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7liaV3Y6AV06",
        "outputId": "989722a7-be4c-40e9-9597-64495bea3c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/images/\n",
            "val: /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/images\n",
            "\n",
            "nc: 3\n",
            "names: [\"Normal\", \"crackles\", \"wheezes\"]\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/data.yaml\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM3WTgM-AhNy"
      },
      "source": [
        "###YoloV9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlce66DnAdUT",
        "outputId": "b8bf3ecb-1e4a-4946-ef93-d353047ef4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 325, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 325 (delta 134), reused 102 (delta 102), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (325/325), 2.26 MiB | 1.09 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n"
          ]
        }
      ],
      "source": [
        "# Download Github rep.\n",
        "!git clone https://github.com/SkalskiP/yolov9.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTTNINAXEymy",
        "outputId": "09834902-f19e-41ac-98a6-ad936f9e92f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/195.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m194.6/195.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Ä°nstall the req.\n",
        "!pip install -r /content/yolov9/requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvsvVbh2E3D2",
        "outputId": "7558afd4-344f-4b5c-d80e-6c328f262091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-15 09:06:29.468402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-15 09:06:29.468493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-15 09:06:29.470846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-15 09:06:30.769818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=, cfg=/content/yolov9/models/detect/yolov9-c.yaml, data=/content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/data.yaml, hyp=/content/yolov9/data/hyps/hyp.scratch-high.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov9/runs/train, name=yolov9-c, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "YOLOv5 ğŸš€ 1e33dbb Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, dfl=1.5, obj_pw=1.0, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO ğŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO ğŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov9/runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 36.9MB/s]\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1         0  models.common.Silence                   []                            \n",
            "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            "  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            "  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            "  8                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            "  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
            " 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
            " 17                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            " 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
            " 20                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \n",
            " 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \n",
            " 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \n",
            " 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            " 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            " 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            " 29                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            " 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \n",
            " 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            " 32                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \n",
            " 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 35                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \n",
            " 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 38[31, 34, 37, 16, 19, 22]  1  21547442  models.yolo.DualDDetect                 [3, [512, 512, 512, 256, 512, 512]]\n",
            "yolov9-c summary: 962 layers, 51004210 parameters, 51004178 gradients, 238.9 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 238 weight(decay=0.0), 255 weight(decay=0.0005), 253 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/train/labels.cache... 4828 images, 6 backgrounds, 0 corrupt: 100% 4828/4828 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/Training/data/val/labels.cache... 1034 images, 0 backgrounds, 0 corrupt: 100% 1034/1034 [00:00<?, ?it/s]\n",
            "Plotting labels to yolov9/runs/train/yolov9-c/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1myolov9/runs/train/yolov9-c\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       0/49         0G      4.414      5.269      5.268         58        640:   0% 0/302 [03:39<?, ?it/s]WARNING âš ï¸ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
            "       0/49         0G       4.66        5.5      5.499         59        640:   4% 12/302 [20:35<8:17:48, 103.00s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov9/train_dual.py\", line 644, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov9/train_dual.py\", line 538, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov9/train_dual.py\", line 314, in train\n",
            "    pred = model(imgs)  # forward\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/yolov9/models/yolo.py\", line 579, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/content/yolov9/models/yolo.py\", line 481, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/yolov9/models/common.py\", line 592, in forward\n",
            "    y.extend((m(y[-1])) for m in [self.cv2, self.cv3])\n",
            "  File \"/content/yolov9/models/common.py\", line 592, in <genexpr>\n",
            "    y.extend((m(y[-1])) for m in [self.cv2, self.cv3])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/yolov9/models/common.py\", line 54, in forward\n",
            "    return self.act(self.bn(self.conv(x)))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Train with YoloV9\n",
        "\"\"\"\n",
        "workers --> max dataloader workers (per RANK in DDP mode)\n",
        "device --> cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "batch --> total batch size for all GPUs, -1 for autobatch\n",
        "epochs --> total training epochs\n",
        "data --> dataset.yaml path\n",
        "img --> train, val image size (pixels)\n",
        "cfg --> model.yaml path\n",
        "weights --> initial weights path\n",
        "name --> save to project/name\n",
        "hyp --> hyperparameters path\n",
        "\n",
        "**This parameters is my project parameters. You can make changes for your project.**\n",
        "\"\"\"\n",
        "\n",
        "!python /content/yolov9/train_dual.py \\\n",
        "--workers 8 --device cpu --batch 16 --epochs 50 --min-items 0 --close-mosaic 15\\\n",
        "--data /content/drive/MyDrive/AIFFEL/AIFFELTON/model/YoloV9/data.yaml \\\n",
        "--img 640 --cfg /content/yolov9/models/detect/yolov9-c.yaml \\\n",
        "--weights '' --name yolov9-c --hyp /content/yolov9/data/hyps/hyp.scratch-high.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iCLKgX5I60x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}